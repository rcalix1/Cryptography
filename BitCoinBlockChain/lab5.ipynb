{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6105e8b6-532f-45c7-a339-45d6c8fae92d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "file_less.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder, StandardScaler\n\u001b[1;32m---> 16\u001b[0m feature \u001b[38;5;241m=\u001b[39m genfromtxt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_less.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, usecols\u001b[38;5;241m=\u001b[39m(i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1001\u001b[39m)), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m, skip_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     17\u001b[0m target \u001b[38;5;241m=\u001b[39m genfromtxt(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_less.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, usecols\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m, skip_header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, feature\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\npyio.py:1980\u001b[0m, in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\u001b[0m\n\u001b[0;32m   1978\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os_fspath(fname)\n\u001b[0;32m   1979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1980\u001b[0m     fid \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39m_datasource\u001b[38;5;241m.\u001b[39mopen(fname, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrt\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39mencoding)\n\u001b[0;32m   1981\u001b[0m     fid_ctx \u001b[38;5;241m=\u001b[39m contextlib\u001b[38;5;241m.\u001b[39mclosing(fid)\n\u001b[0;32m   1982\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mopen(path, mode, encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\lib\\_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[1;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    531\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: file_less.csv not found."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from numpy import genfromtxt\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, confusion_matrix, f1_score, precision_score, recall_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "\n",
    "feature = genfromtxt('file_less.csv', delimiter=',', usecols=(i for i in range(1,1001)), dtype=int, skip_header=1)\n",
    "target = genfromtxt('file_less.csv', delimiter=',', usecols=(0), dtype=int, skip_header=1)\n",
    "\n",
    "print(\"Feature shape:\", feature.shape)\n",
    "print(\"Target shape:\", target.shape)\n",
    "\n",
    "labels = LabelEncoder().fit_transform(target)\n",
    "feature_std = StandardScaler().fit_transform(feature)\n",
    "x_train, x_test, y_train, y_test = train_test_split(feature_std, labels, test_size=0.25, random_state=0)\n",
    "\n",
    "print(\"Training set shape:\", x_train.shape)\n",
    "print(\"Testing set shape:\", x_test.shape)\n",
    "\n",
    "def print_stats_metrics(y_test, y_pred):\n",
    "    print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
    "    confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confmat)\n",
    "    print(\"\\nCrosstab:\")\n",
    "    print(pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "    print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred, average='binary'))\n",
    "    print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))\n",
    "    print('F1-measure: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "# Logistic Regression\n",
    "print(\"\\n======================== Logistic Regression ========================\")\n",
    "clfLog = LogisticRegression()\n",
    "clfLog.fit(x_train, y_train)\n",
    "predictions = clfLog.predict(x_test)\n",
    "print_stats_metrics(y_test, predictions)\n",
    "\n",
    "# Random Forest\n",
    "print(\"\\n======================== Random Forest ========================\")\n",
    "clfRandForest = RandomForestClassifier()\n",
    "clfRandForest.fit(x_train, y_train)\n",
    "predictions = clfRandForest.predict(x_test)\n",
    "print_stats_metrics(y_test, predictions)\n",
    "\n",
    "# Decision Tree\n",
    "print(\"\\n======================== Decision Tree ========================\")\n",
    "clfDT = DecisionTreeRegressor()\n",
    "clfDT.fit(x_train, y_train)\n",
    "predictions = clfDT.predict(x_test)\n",
    "print_stats_metrics(y_test, predictions.round())\n",
    "\n",
    "# Naive Bayes\n",
    "print(\"\\n======================== Naive Bayes ========================\")\n",
    "clfNB = GaussianNB()\n",
    "clfNB.fit(x_train, y_train)\n",
    "predictions = clfNB.predict(x_test)\n",
    "print_stats_metrics(y_test, predictions)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def layer(input, weight_shape, bias_shape):\n",
    "    weight_stddev = (2.0/weight_shape[0])**0.5\n",
    "    w_init = tf.random_normal_initializer(stddev=weight_stddev)\n",
    "    bias_init = tf.constant_initializer(value=0)\n",
    "    W = tf.get_variable(\"W\", weight_shape, initializer=w_init)\n",
    "    b = tf.get_variable(\"b\", bias_shape, initializer=bias_init)\n",
    "    return tf.nn.relu(tf.matmul(input, W) + b)\n",
    "\n",
    "def inference_deep_layers(x_tf, n_features, n_columns):\n",
    "    with tf.variable_scope(\"hidden_1\"):\n",
    "        hidden_1 = layer(x_tf, [n_features, 30], [30])\n",
    "    with tf.variable_scope(\"hidden_2\"):\n",
    "        hidden_2 = layer(hidden_1, [30, 25], [25])\n",
    "    with tf.variable_scope(\"hidden_3\"):\n",
    "        hidden_3 = layer(hidden_2, [25, 10], [10])\n",
    "    with tf.variable_scope(\"hidden_4\"):\n",
    "        hidden_4 = layer(hidden_3, [10, 5], [5])\n",
    "    with tf.variable_scope(\"output\"):\n",
    "        output = layer(hidden_4, [5, n_columns], [n_columns])\n",
    "    return output\n",
    "\n",
    "def loss_deep(output, y_tf):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y_tf)\n",
    "    loss = tf.reduce_mean(xentropy)\n",
    "    return loss\n",
    "\n",
    "def training(cost, learning_rate):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(cost)\n",
    "    return train_op\n",
    "\n",
    "def evaluate(output, y_tf):\n",
    "    correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(y_tf, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    return accuracy\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y_train_onehot = onehot_encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_onehot = onehot_encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Set parameters\n",
    "learning_rate = 0.001\n",
    "n_epochs = 5000\n",
    "batch_size = 32\n",
    "\n",
    "# Create placeholders\n",
    "x_tf = tf.placeholder(\"float\", [None, x_train.shape[1]])\n",
    "y_tf = tf.placeholder(\"float\", [None, y_train_onehot.shape[1]])\n",
    "\n",
    "# Build the graph\n",
    "output = inference_deep_layers(x_tf, x_train.shape[1], y_train_onehot.shape[1])\n",
    "cost = loss_deep(output, y_tf)\n",
    "train_op = training(cost, learning_rate)\n",
    "eval_op = evaluate(output, y_tf)\n",
    "\n",
    "# Initialize variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Run the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(len(x_train) / batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_x = x_train[i*batch_size:(i+1)*batch_size]\n",
    "            batch_y = y_train_onehot[i*batch_size:(i+1)*batch_size]\n",
    "            _, c = sess.run([train_op, cost], feed_dict={x_tf: batch_x, y_tf: batch_y})\n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        if epoch % 500 == 0:\n",
    "            train_acc = sess.run(eval_op, feed_dict={x_tf: x_train, y_tf: y_train_onehot})\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs}, Cost: {avg_cost:.5f}, Train Acc: {train_acc:.3f}\")\n",
    "    \n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    # Test the model\n",
    "    test_acc = sess.run(eval_op, feed_dict={x_tf: x_test, y_tf: y_test_onehot})\n",
    "    print(f\"Test Accuracy: {test_acc:.3f}\")\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = sess.run(tf.argmax(output, 1), feed_dict={x_tf: x_test})\n",
    "    print_stats_metrics(y_test, predictions)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b955cb4-d4b8-416d-bfd3-e9cdd917df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats_metrics(y_test, y_pred):\n",
    "    print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))\n",
    "    confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confmat)\n",
    "    print(\"\\nCrosstab:\")\n",
    "    print(pd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "    print('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred, average='binary'))\n",
    "    print('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))\n",
    "    print('F1-measure: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e01ff8-b4f0-4232-b741-88a0601d288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
